{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Wrangle Report</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<t><center><h3>by Vrushabh Suchak</h3></center><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This report is for the Data Wrangling project completed as a part of Data Analyst Nanodegree by Udacity. The dataset that is wrangled is the tweet archive of Twitter user [@dog_rates](https://twitter.com/dog_rates), also known as `WeRateDogs`. `WeRateDogs` is a Twitter account that rates people's dogs with a humorous comment about the dog. These ratings almost always have a denominator of 10. The numerator, however, are always greater than 10.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Details "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Following were the tasks of this project:**\n",
    "- Gathering Data \n",
    "- Assessing Data \n",
    "- Cleaning Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for this project consists of three different datasets that were obtained as following:\n",
    "1. <b> Twitter archive file: </b> The `twitter_archive_enhanced.csv` file was provided by Udacity and was downloaded manually.\n",
    "2. <b> The tweet image predictions: </b> The `image_predictions.tsv` file has information about what breed of dog is present in each tweet according to a neural network. This file is hosted on Udacity's servers and was downloaded programmatically using the Requests library and the given URL.\n",
    "3. <b> Twitter API & JSON: </b> By using the tweet IDs in the WeRateDogs Twitter archive file, I queried the Twitter API for each tweet's JSON data using Python's Tweepy library and stored each tweet's entire set of JSON data in a file called `tweet_json.txt` file. I read this .txt file line by line into a pandas dataframe with tweet ID, favorite count, retweet count, followers count, friends count, source, retweeted status and url. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After gathering each of the above pieces of data, I assessed the data as following:\n",
    "1. <b>Visually:</b> The above pieces of data were assessed visually by printing the three entire dataframes separate in Jupyter Notebook.\n",
    "2. <b>Programmatically: </b> The above pieces of data were assessed programmatically by using different methods (e.g. info, value_counts, sample, duplicated, groupby, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I separated the issues encountered in quality issues and tidiness issues. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting the cleaning process, I made a copy of each dataframe. All of the cleaning operations were conducted on this copy so we can still view the original dirty and/or messy dataset later. This part of Data Wrangling was divided in three parts: `Define, Code, and Test the code`. <br><br>\n",
    "These three steps were performed on each of the issues described while assessing the data. First the Quality and Tidiness issues were solved on each dataframe and then all these dataframes were merged to form one single dataframe. <br><br> \n",
    "This dataframe is exported to csv as `twitter_archive_master.csv`. This dataframe was eventually used for analyzing and visualizing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the gathering, assessing and cleaning process, we get high quality and tidy master pandas DataFrame which is stored as `twitter_archive_master.csv` which was further used for analysis and visualization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
